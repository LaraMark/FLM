# Define the root directory for the image dataset
image_root = '/export/share/datasets/vision/nocaps/'

# Define the root directory for the image annotations
ann_root = 'annotation'

# Set the pretrained model file path or URL
# The pretrained model will be used for captioning
pretrained = 'https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_base_caption_capfilt_large.pth'

# Specify the vision transformer model type
# The options are 'base', 'large', or 'huge'
vit = 'base'

# Set the batch size for processing images
batch_size = 32

# Define the image size for the vision transformer model
image_size = 384

# Set the maximum and minimum length for the generated captions
max_length = 20
min_length = 5

# Set the number of beams for beam search
# This parameter controls the diversity and quality of the generated captions
num_beams = 3

# Define the prompt for generating captions
# The generated captions will start with this prompt
prompt = 'a picture of '
